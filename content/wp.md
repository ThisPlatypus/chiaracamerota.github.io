
---
title: "Work in Progress"
description: "My current reading list."
---
<div class="image-container">
    <img src="https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExMWwza3EyY3hqYmY4aHd2enUxMzU1enZzNW91Nm51bmFwYjRlZnVheSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3oz8xTUmZABI3PGwDe/giphy.gif" alt="Image 1"></div>

##### Work in Progress


- **Load Profile Generation for Robust Optimization:  A Stochastic Approach Based on Conditional Probability Approximation** (Joint with L. Becchi, M. Intraviata, M. Bindi, and G. M. Lozito) -- [Under submission: 25th EEEIC International Conference on Environment and Electrical Engineering & 9th I&CPS Industrial and Commercial Power Systems Europe ]

    _ABSTRACT: With the growing integration of renewable energy sources into distributed grids, accurate household‐level load forecasting becomes essential for robust energy management and optimization. This paper proposes a lightweight stochastic profile generation method grounded in conditional probability approximation. First, empirical conditional distributions are mined from historical load data via hourly histogram binning and correlation analysis. Second, a Monte Carlo‐inspired “flock” of plausible future load trajectories is generated iteratively, each endowed with an occurrence probability. Validation on the Ausgrid dataset (127 prosumer profiles over one year) shows that the probabilistic mining step requires only 0.5–0.6 s for history depths of 30–180 days, while generating 200 scenarios takes merely 8.1 ms, with a total memory footprint of approximately 200 KB. These computational and storage efficiencies render the approach suitable for online deployment on edge devices, enabling robust optimization under uncertainty in renewable energy communities._

  
-  **Communication-Efficient Federated Learning with Top K selection** (Joint with Javier Palomares, Estefanía Coronado and Flavio Esposito) -- [Under submission: ACM SIGCOMM 2025]
      _ABSTRACT: Communication bottlenecks hinder faster convergence in Federated Learning (FL) architectures. Several model compression techniques have been proposed to cope with this problem, e.g., sparsification or quantization. Although sound, these solutions are network-agnostic and rarely consider each worker's contributions to learning convergence.   
To this aim, we introduce SCALP, a network-aware compression strategy that reduces gradient exchange overhead by selectively transmitting the most informative updates based on the statistical deviation of each worker's contribution and current bandwidth conditions. Workers locally adapt compression levels using a probabilistic filtering strategy and encode this state with minimal overhead into the TCP header using 2-bit ECN signaling. We evaluated SCALP on CNN and CNN-LSTM models on the CMAPSS data set, and show how we gain a reduction in communication cost by more than 25\%,  maintaining convergence performance comparable to centralized training at the beginning of the study._
  
    <div class="image-container"><img src="https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExa2ZtdTliMm9wcnNpYXZwenA2YWdmazMxYmhkaTFodHloZHd3cGptbSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/lizSDX8mHfbstV0GKw/giphy.gif" alt="Image 2">
</div>

